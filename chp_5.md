# Scaling Up Training 

<img src='./asserts/5_1.png'></img>

<img src='./asserts/5_2.png'></img>

<img src='./asserts/5_3.png'></img>

<img src='./asserts/5_4.png'></img>

<img src='./asserts/5_5.png'></img>

a larger model based on previous one.

<img src='./asserts/5_6.png'></img>

<img src='./asserts/5_7.png'></img>

<img src='./asserts/5_8.png'></img>

# Mixed-Precision Training 

<img src='./asserts/5_9.png'></img>

too small part just put them into zero.

[MIXED PRECISION TRAINING](https://arxiv.org/pdf/1710.03740.pdf)

# Distributed

<img src='./asserts/5_10.png'></img>

fragment (n.) 分段

# Distributed pipeline or distributed tensor

<img src='./asserts/5_11.png'></img>

# Storing Activation

<img src='./asserts/5_12.png'></img>

TBD : slide 22

and Pytorch
